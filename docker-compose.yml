services:
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    ports:
      - "3000:8080"
    volumes:
      - open-webui:/app/backend/data
    environment:
      # Disable Ollama since we're using LM Studio or cloud APIs
      - ENABLE_OLLAMA_API=false
      # Enable OpenAI API for cloud providers and LM Studio
      - ENABLE_OPENAI_API=true
      # Allow signup for first user (becomes admin)
      - WEBUI_AUTH=true
      #
      # === OPTIONAL: LM Studio Connection ===
      # Uncomment the lines below if you want to connect to LM Studio:
      #
      # - OPENAI_API_BASE_URL=http://host.docker.internal:1234/v1
      # - OPENAI_API_KEY=lm-studio
      #
      # Otherwise, add your API keys through the web UI:
      # Settings → Admin Settings → Connections
      #
    extra_hosts:
      - "host.docker.internal:host-gateway"
    restart: unless-stopped

volumes:
  open-webui:
